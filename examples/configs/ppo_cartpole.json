{
  "log_level": "info",

  "batch_size": 4000,
  "discount": 0.99,
  "gae_rewards": false,
  "gae_lambda": 0.97,
  "normalize_advantage" : false,
  "learning_rate" : 0.001,
  "entropy_penalty" :  0.01,
  "epochs" : 5,
  "optimizer_batch_size" : 512,
  "loss_clipping" : 0.2,

  "baseline": {
    "type": "mlp",
    "sizes": [32, 32],
    "epochs": 1,
    "update_batch_size" : 512,
    "learning_rate": 0.01
  }
}
